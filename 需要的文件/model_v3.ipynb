{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506ce59a-7cab-4246-8921-890f594c029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from functools import partial\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "\n",
    "def _make_divisible(ch, divisor=8, min_ch=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    \"\"\"\n",
    "    if min_ch is None:\n",
    "        min_ch = divisor\n",
    "    new_ch = max(min_ch, int(ch + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_ch < 0.9 * ch:\n",
    "        new_ch += divisor\n",
    "    return new_ch\n",
    "\n",
    "\n",
    "def correct_pad(input_size: Union[int, tuple], kernel_size: int):\n",
    "    \"\"\"Returns a tuple for zero-padding for 2D convolution with downsampling.\n",
    "    Arguments:\n",
    "      input_size: Input tensor size.\n",
    "      kernel_size: An integer or tuple/list of 2 integers.\n",
    "    Returns:\n",
    "      A tuple.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(input_size, int):\n",
    "        input_size = (input_size, input_size)\n",
    "\n",
    "    kernel_size = (kernel_size, kernel_size)\n",
    "\n",
    "    adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)\n",
    "    correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
    "    return ((correct[0] - adjust[0], correct[0]),\n",
    "            (correct[1] - adjust[1], correct[1]))\n",
    "\n",
    "\n",
    "class HardSigmoid(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(HardSigmoid, self).__init__(**kwargs)\n",
    "        self.relu6 = layers.ReLU(6.)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.relu6(inputs + 3) * (1. / 6)\n",
    "        return x\n",
    "\n",
    "\n",
    "class HardSwish(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(HardSwish, self).__init__(**kwargs)\n",
    "        self.hard_sigmoid = HardSigmoid()\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.hard_sigmoid(inputs) * inputs\n",
    "        return x\n",
    "\n",
    "\n",
    "def _se_block(inputs, filters, prefix, se_ratio=1 / 4.):\n",
    "    # [batch, height, width, channel] -> [batch, channel]\n",
    "    x = layers.GlobalAveragePooling2D(name=prefix + 'squeeze_excite/AvgPool')(inputs)\n",
    "\n",
    "    # Target shape. Tuple of integers, does not include the samples dimension (batch size).\n",
    "    # [batch, channel] -> [batch, 1, 1, channel]\n",
    "    x = layers.Reshape((1, 1, filters))(x)\n",
    "\n",
    "    # fc1\n",
    "    x = layers.Conv2D(filters=_make_divisible(filters * se_ratio),\n",
    "                      kernel_size=1,\n",
    "                      padding='same',\n",
    "                      name=prefix + 'squeeze_excite/Conv')(x)\n",
    "    x = layers.ReLU(name=prefix + 'squeeze_excite/Relu')(x)\n",
    "\n",
    "    # fc2\n",
    "    x = layers.Conv2D(filters=filters,\n",
    "                      kernel_size=1,\n",
    "                      padding='same',\n",
    "                      name=prefix + 'squeeze_excite/Conv_1')(x)\n",
    "    x = HardSigmoid(name=prefix + 'squeeze_excite/HardSigmoid')(x)\n",
    "\n",
    "    x = layers.Multiply(name=prefix + 'squeeze_excite/Mul')([inputs, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def _inverted_res_block(x,\n",
    "                        input_c: int,      # input channel\n",
    "                        kernel_size: int,  # kennel size\n",
    "                        exp_c: int,        # expanded channel\n",
    "                        out_c: int,        # out channel\n",
    "                        use_se: bool,      # whether using SE\n",
    "                        activation: str,   # RE or HS\n",
    "                        stride: int,\n",
    "                        block_id: int,\n",
    "                        alpha: float = 1.0):\n",
    "\n",
    "    bn = partial(layers.BatchNormalization, epsilon=0.001, momentum=0.99)\n",
    "\n",
    "    input_c = _make_divisible(input_c * alpha)\n",
    "    exp_c = _make_divisible(exp_c * alpha)\n",
    "    out_c = _make_divisible(out_c * alpha)\n",
    "\n",
    "    act = layers.ReLU if activation == \"RE\" else HardSwish\n",
    "\n",
    "    shortcut = x\n",
    "    prefix = 'expanded_conv/'\n",
    "    if block_id:\n",
    "        # expand channel\n",
    "        prefix = 'expanded_conv_{}/'.format(block_id)\n",
    "        x = layers.Conv2D(filters=exp_c,\n",
    "                          kernel_size=1,\n",
    "                          padding='same',\n",
    "                          use_bias=False,\n",
    "                          name=prefix + 'expand')(x)\n",
    "        x = bn(name=prefix + 'expand/BatchNorm')(x)\n",
    "        x = act(name=prefix + 'expand/' + act.__name__)(x)\n",
    "\n",
    "    if stride == 2:\n",
    "        input_size = (x.shape[1], x.shape[2])  # height, width\n",
    "        x = layers.ZeroPadding2D(padding=correct_pad(input_size, kernel_size),\n",
    "                                 name=prefix + 'depthwise/pad')(x)\n",
    "\n",
    "    x = layers.DepthwiseConv2D(kernel_size=kernel_size,\n",
    "                               strides=stride,\n",
    "                               padding='same' if stride == 1 else 'valid',\n",
    "                               use_bias=False,\n",
    "                               name=prefix + 'depthwise')(x)\n",
    "    x = bn(name=prefix + 'depthwise/BatchNorm')(x)\n",
    "    x = act(name=prefix + 'depthwise/' + act.__name__)(x)\n",
    "\n",
    "    if use_se:\n",
    "        x = _se_block(x, filters=exp_c, prefix=prefix)\n",
    "\n",
    "    x = layers.Conv2D(filters=out_c,\n",
    "                      kernel_size=1,\n",
    "                      padding='same',\n",
    "                      use_bias=False,\n",
    "                      name=prefix + 'project')(x)\n",
    "    x = bn(name=prefix + 'project/BatchNorm')(x)\n",
    "\n",
    "    if stride == 1 and input_c == out_c:\n",
    "        x = layers.Add(name=prefix + 'Add')([shortcut, x])\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def mobilenet_v3_large(input_shape=(224, 224, 3),\n",
    "                       num_classes=1000,\n",
    "                       alpha=1.0,\n",
    "                       include_top=True):\n",
    "    \"\"\"\n",
    "    download weights url:\n",
    "    链接: https://pan.baidu.com/s/13uJznKeqHkjUp72G_gxe8Q  密码: 8quu\n",
    "    \"\"\"\n",
    "    bn = partial(layers.BatchNormalization, epsilon=0.001, momentum=0.99)\n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv2D(filters=16,\n",
    "                      kernel_size=3,\n",
    "                      strides=(2, 2),\n",
    "                      padding='same',\n",
    "                      use_bias=False,\n",
    "                      name=\"Conv\")(img_input)\n",
    "    x = bn(name=\"Conv/BatchNorm\")(x)\n",
    "    x = HardSwish(name=\"Conv/HardSwish\")(x)\n",
    "\n",
    "    inverted_cnf = partial(_inverted_res_block, alpha=alpha)\n",
    "    # input, input_c, k_size, expand_c, use_se, activation, stride, block_id\n",
    "    x = inverted_cnf(x, 16, 3, 16, 16, False, \"RE\", 1, 0)\n",
    "    x = inverted_cnf(x, 16, 3, 64, 24, False, \"RE\", 2, 1)\n",
    "    x = inverted_cnf(x, 24, 3, 72, 24, False, \"RE\", 1, 2)\n",
    "    x = inverted_cnf(x, 24, 5, 72, 40, True, \"RE\", 2, 3)\n",
    "    x = inverted_cnf(x, 40, 5, 120, 40, True, \"RE\", 1, 4)\n",
    "    x = inverted_cnf(x, 40, 5, 120, 40, True, \"RE\", 1, 5)\n",
    "    x = inverted_cnf(x, 40, 3, 240, 80, False, \"HS\", 2, 6)\n",
    "    x = inverted_cnf(x, 80, 3, 200, 80, False, \"HS\", 1, 7)\n",
    "    x = inverted_cnf(x, 80, 3, 184, 80, False, \"HS\", 1, 8)\n",
    "    x = inverted_cnf(x, 80, 3, 184, 80, False, \"HS\", 1, 9)\n",
    "    x = inverted_cnf(x, 80, 3, 480, 112, True, \"HS\", 1, 10)\n",
    "    x = inverted_cnf(x, 112, 3, 672, 112, True, \"HS\", 1, 11)\n",
    "    x = inverted_cnf(x, 112, 5, 672, 160, True, \"HS\", 2, 12)\n",
    "    x = inverted_cnf(x, 160, 5, 960, 160, True, \"HS\", 1, 13)\n",
    "    x = inverted_cnf(x, 160, 5, 960, 160, True, \"HS\", 1, 14)\n",
    "\n",
    "    last_c = _make_divisible(160 * 6 * alpha)\n",
    "    last_point_c = _make_divisible(1280 * alpha)\n",
    "\n",
    "    x = layers.Conv2D(filters=last_c,\n",
    "                      kernel_size=1,\n",
    "                      padding='same',\n",
    "                      use_bias=False,\n",
    "                      name=\"Conv_1\")(x)\n",
    "    x = bn(name=\"Conv_1/BatchNorm\")(x)\n",
    "    x = HardSwish(name=\"Conv_1/HardSwish\")(x)\n",
    "\n",
    "    if include_top is True:\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Reshape((1, 1, last_c))(x)\n",
    "\n",
    "        # fc1\n",
    "        x = layers.Conv2D(filters=last_point_c,\n",
    "                          kernel_size=1,\n",
    "                          padding='same',\n",
    "                          name=\"Conv_2\")(x)\n",
    "        x = HardSwish(name=\"Conv_2/HardSwish\")(x)\n",
    "\n",
    "        # fc2\n",
    "        x = layers.Conv2D(filters=num_classes,\n",
    "                          kernel_size=1,\n",
    "                          padding='same',\n",
    "                          name='Logits/Conv2d_1c_1x1')(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Softmax(name=\"Predictions\")(x)\n",
    "\n",
    "    model = Model(img_input, x, name=\"MobilenetV3large\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def mobilenet_v3_small(input_shape=(224, 224, 3),\n",
    "                       num_classes=1000,\n",
    "                       alpha=1.0,\n",
    "                       include_top=True):\n",
    "    \"\"\"\n",
    "    download weights url:\n",
    "    链接: https://pan.baidu.com/s/1vrQ_6HdDTHL1UUAN6nSEcw  密码: rrf0\n",
    "    \"\"\"\n",
    "    bn = partial(layers.BatchNormalization, epsilon=0.001, momentum=0.99)\n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv2D(filters=16,\n",
    "                      kernel_size=3,\n",
    "                      strides=(2, 2),\n",
    "                      padding='same',\n",
    "                      use_bias=False,\n",
    "                      name=\"Conv\")(img_input)\n",
    "    x = bn(name=\"Conv/BatchNorm\")(x)\n",
    "    x = HardSwish(name=\"Conv/HardSwish\")(x)\n",
    "\n",
    "    inverted_cnf = partial(_inverted_res_block, alpha=alpha)\n",
    "    # input, input_c, k_size, expand_c, use_se, activation, stride, block_id\n",
    "    x = inverted_cnf(x, 16, 3, 16, 16, True, \"RE\", 2, 0)\n",
    "    x = inverted_cnf(x, 16, 3, 72, 24, False, \"RE\", 2, 1)\n",
    "    x = inverted_cnf(x, 24, 3, 88, 24, False, \"RE\", 1, 2)\n",
    "    x = inverted_cnf(x, 24, 5, 96, 40, True, \"HS\", 2, 3)\n",
    "    x = inverted_cnf(x, 40, 5, 240, 40, True, \"HS\", 1, 4)\n",
    "    x = inverted_cnf(x, 40, 5, 240, 40, True, \"HS\", 1, 5)\n",
    "    x = inverted_cnf(x, 40, 5, 120, 48, True, \"HS\", 1, 6)\n",
    "    x = inverted_cnf(x, 48, 5, 144, 48, True, \"HS\", 1, 7)\n",
    "    x = inverted_cnf(x, 48, 5, 288, 96, True, \"HS\", 2, 8)\n",
    "    x = inverted_cnf(x, 96, 5, 576, 96, True, \"HS\", 1, 9)\n",
    "    x = inverted_cnf(x, 96, 5, 576, 96, True, \"HS\", 1, 10)\n",
    "\n",
    "    last_c = _make_divisible(96 * 6 * alpha)\n",
    "    last_point_c = _make_divisible(1024 * alpha)\n",
    "\n",
    "    x = layers.Conv2D(filters=last_c,\n",
    "                      kernel_size=1,\n",
    "                      padding='same',\n",
    "                      use_bias=False,\n",
    "                      name=\"Conv_1\")(x)\n",
    "    x = bn(name=\"Conv_1/BatchNorm\")(x)\n",
    "    x = HardSwish(name=\"Conv_1/HardSwish\")(x)\n",
    "\n",
    "    if include_top is True:\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Reshape((1, 1, last_c))(x)\n",
    "\n",
    "        # fc1\n",
    "        x = layers.Conv2D(filters=last_point_c,\n",
    "                          kernel_size=1,\n",
    "                          padding='same',\n",
    "                          name=\"Conv_2\")(x)\n",
    "        x = HardSwish(name=\"Conv_2/HardSwish\")(x)\n",
    "\n",
    "        # fc2\n",
    "        x = layers.Conv2D(filters=num_classes,\n",
    "                          kernel_size=1,\n",
    "                          padding='same',\n",
    "                          name='Logits/Conv2d_1c_1x1')(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Softmax(name=\"Predictions\")(x)\n",
    "\n",
    "    model = Model(img_input, x, name=\"MobilenetV3large\")\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
